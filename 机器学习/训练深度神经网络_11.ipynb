{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 27.6197 - accuracy: 0.0871 - val_loss: 27.5955 - val_accuracy: 0.0814\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 27.6166 - accuracy: 0.0833 - val_loss: 27.5936 - val_accuracy: 0.0880\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 27.6154 - accuracy: 0.0917 - val_loss: 27.5926 - val_accuracy: 0.0988\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 27.6147 - accuracy: 0.0953 - val_loss: 27.5920 - val_accuracy: 0.1050\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 996us/step - loss: 27.6143 - accuracy: 0.0979 - val_loss: 27.5917 - val_accuracy: 0.1074\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 27.6139 - accuracy: 0.0983 - val_loss: 27.5914 - val_accuracy: 0.1090\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 27.6137 - accuracy: 0.1001 - val_loss: 27.5912 - val_accuracy: 0.1116\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 27.6136 - accuracy: 0.1008 - val_loss: 27.5910 - val_accuracy: 0.1102\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 27.6134 - accuracy: 0.1013 - val_loss: 27.5908 - val_accuracy: 0.1108\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 27.6133 - accuracy: 0.1015 - val_loss: 27.5907 - val_accuracy: 0.1104\n"
     ]
    }
   ],
   "source": [
    "#梯度消失和梯度爆炸\n",
    "from tensorflow import keras\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
    "print(x_train.shape)\n",
    "x_train,x_valid = x_train[5000:]/255.0,x_train[:5000]/255.0\n",
    "y_train,y_valid = y_train[5000:],y_train[:5000]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(30,activation='relu',input_shape=x_train.shape[1:]),\n",
    "    keras.layers.Dense(10,activation='softmax')\n",
    "])\n",
    "\n",
    "#非饱和激活函数\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(30,kernel_initializer='he_normal'),    #初始化方式\n",
    "    keras.layers.LeakyReLU(alpha=0.2),                        #PReLU激活函数 \n",
    "    #keras.layers.Dense(30,activation='selu',kernel_initializer='lecun_normal')\n",
    "    keras.layers.Dense(10,activation='softmax')\n",
    "])\n",
    "model.compile(loss='mean_squared_error',optimizer='sgd',metrics=['accuracy'])#回归不用metrics=['accuracy']\n",
    "\n",
    "\n",
    "#批量归一化\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(30,activation='relu',input_shape=x_train.shape[1:]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10,activation='softmax')\n",
    "])\n",
    "\n",
    "#梯度裁剪\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)     #clipnorm = 1.0\n",
    "model.compile(loss='mean_squared_error',optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(x_train,y_train,epochs=10,validation_data=(x_valid,y_valid))   #validation_split =0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "输出的种类数： 10\n",
      "Epoch 1/4\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5504 - accuracy: 0.8418 - val_loss: 0.3963 - val_accuracy: 0.8684\n",
      "Epoch 2/4\n",
      "1719/1719 [==============================] - 2s 969us/step - loss: 0.3664 - accuracy: 0.8770 - val_loss: 0.3671 - val_accuracy: 0.8748\n",
      "Epoch 3/4\n",
      "1719/1719 [==============================] - 2s 928us/step - loss: 0.3448 - accuracy: 0.8810 - val_loss: 0.3542 - val_accuracy: 0.8762\n",
      "Epoch 4/4\n",
      "1719/1719 [==============================] - 2s 929us/step - loss: 0.3357 - accuracy: 0.8820 - val_loss: 0.3504 - val_accuracy: 0.8796\n",
      "Epoch 1/16\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3301 - accuracy: 0.8837 - val_loss: 0.3475 - val_accuracy: 0.8790\n",
      "Epoch 2/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3293 - accuracy: 0.8833 - val_loss: 0.3473 - val_accuracy: 0.8800\n",
      "Epoch 3/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3287 - accuracy: 0.8839 - val_loss: 0.3469 - val_accuracy: 0.8782\n",
      "Epoch 4/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3283 - accuracy: 0.8838 - val_loss: 0.3468 - val_accuracy: 0.8798\n",
      "Epoch 5/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3278 - accuracy: 0.8842 - val_loss: 0.3463 - val_accuracy: 0.8794\n",
      "Epoch 6/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3275 - accuracy: 0.8841 - val_loss: 0.3461 - val_accuracy: 0.8778\n",
      "Epoch 7/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3271 - accuracy: 0.8846 - val_loss: 0.3463 - val_accuracy: 0.8786\n",
      "Epoch 8/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3267 - accuracy: 0.8845 - val_loss: 0.3455 - val_accuracy: 0.8790\n",
      "Epoch 9/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3264 - accuracy: 0.8842 - val_loss: 0.3455 - val_accuracy: 0.8794\n",
      "Epoch 10/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3261 - accuracy: 0.8846 - val_loss: 0.3454 - val_accuracy: 0.8800\n",
      "Epoch 11/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3259 - accuracy: 0.8845 - val_loss: 0.3450 - val_accuracy: 0.8778\n",
      "Epoch 12/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3255 - accuracy: 0.8848 - val_loss: 0.3449 - val_accuracy: 0.8772\n",
      "Epoch 13/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3253 - accuracy: 0.8845 - val_loss: 0.3447 - val_accuracy: 0.8774\n",
      "Epoch 14/16\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3250 - accuracy: 0.8848 - val_loss: 0.3445 - val_accuracy: 0.8776\n",
      "Epoch 15/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3248 - accuracy: 0.8848 - val_loss: 0.3445 - val_accuracy: 0.8772\n",
      "Epoch 16/16\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3245 - accuracy: 0.8848 - val_loss: 0.3447 - val_accuracy: 0.8776\n"
     ]
    }
   ],
   "source": [
    "#利用keras迁移学习\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
    "print(x_train.shape)\n",
    "x_train,x_valid = x_train[5000:]/255.0,x_train[:5000]/255.0\n",
    "y_train,y_valid = y_train[5000:],y_train[:5000]\n",
    "print('输出的种类数：',len(np.unique(y_valid)))\n",
    "\n",
    "# model_a = keras.models.Sequential()\n",
    "# model_a.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "# model_a.add(keras.layers.Dense(300,activation='relu'))\n",
    "# model_a.add(keras.layers.Dense(100,activation='relu'))\n",
    "# model_a.add(keras.layers.Dense(10,activation='softmax'))\n",
    "# model_a.compile(loss = 'sparse_categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "# history=model_a.fit(x_train,y_train,epochs=10,validation_data=(x_valid,y_valid))   #validation_split =0.1\n",
    "# model_a.save('my_keras_model.h5')\n",
    "\n",
    "model_a = keras.models.load_model('my_keras_model.h5')\n",
    "model_a_clone = keras.models.clone_model(model_a)\n",
    "model_a_clone.set_weights(model_a.get_weights())\n",
    "model_b_on_a = keras.models.Sequential(model_a_clone.layers[:-1])\n",
    "model_b_on_a.add(keras.layers.Dense(10,activation='softmax'))\n",
    "\n",
    "for layer in model_b_on_a.layers[:-1]:           #冻结层\n",
    "    layer.trainable = False\n",
    "model_b_on_a.compile(loss='sparse_categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "history = model_b_on_a.fit(x_train,y_train,epochs=4,validation_data=(x_valid,y_valid))\n",
    "\n",
    "for layer in model_b_on_a.layers[:-1]:          #解冻层\n",
    "    layer.trainable = True\n",
    "optimizer = keras.optimizers.SGD(lr=1e-4)\n",
    "model_b_on_a.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "history = model_b_on_a.fit(x_train,y_train,epochs=16,validation_data=(x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更快的优化器\n",
    "optimizer = keras.optimizers.SGD(lr=0.001,momentum=0.9)    #动量优化 \n",
    "optimizer = keras.optimizers.SGD(lr=0.001,momentum=0.9,nesterov=True)   #Nesterov加速梯度\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001,rho=0.9)   #RMSprop\n",
    "optimizer = keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999)\n",
    "\n",
    "\n",
    "\n",
    "#学习率调度\n",
    "optimizer =keras.optimizers.SGD(lr=0.01,decay=1e-4)  #幂调度\n",
    "\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01*0.1**(epoch/20)\n",
    "def exponential_decay(lro,s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lro*0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "exponential_decay_fn = exponential_decay(lro=0.01,s=20)\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(x_train_scaled,y_train,[...],callbacks=[lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正则化避免过拟合 \n",
    "\n",
    "#L1和L2 正则化 \n",
    "#如果想要稀疏模型用L1，许多权重为0；L2用于约束神经网络的连接权重\n",
    "layer = keras.layers.Dense(100,activation='elu',kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01))  #keras.regularizers.l1()   keras.regularizers.l1_l2()  \n",
    "\n",
    "\n",
    "#Dropout \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300,activation='elu',kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10,activation='softmax')\n",
    "])\n",
    "\n",
    "#MC Dropout\n",
    "y_probas = np.stack([model(x_test_scaled,training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "\n",
    "np.round(model.predict(x_test_scaled[:1]),2)  #没有丢弃层\n",
    "np.round(y_probas[:,:1],2)   #Dropout\n",
    "np.round(y_proba[:1],2)      #MC Dropout    #适用于构建风险敏感的应用  \n",
    "\n",
    "#最大范数正则化 \n",
    "keras.layers.Dense(100,activation='elu',kernel_initializer='he_normal',kernel_constraint=keras.constraints.max_norm(1.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
